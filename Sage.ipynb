{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/IJpeDhGmYSYUVP6x0w9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guicaettano/sage/blob/main/Sage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bR4pHaPiEccG"
      },
      "outputs": [],
      "source": [
        "!pip install langchain transformers torch streamlit\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Configurando o dispositivo (GPU se disponível, caso contrário CPU)\n",
        "dispositivo = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Carregando o modelo e o tokenizador do DeepSeek Qwen 1.5B\n",
        "modelo_nome = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_nome)\n",
        "modelo = AutoModelForCausalLM.from_pretrained(modelo_nome).to(dispositivo)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3aBw-ycvFN_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "\n"
      ],
      "metadata": {
        "id": "cZ0ZcOEeHz0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Template de Prompt para o LangChain\n",
        "template = PromptTemplate(\n",
        "    input_variables=[\"pergunta\"],\n",
        "    template=\"Você é um chatbot inteligente e amigável. Responda com clareza e objetividade: {pergunta}\"\n",
        ")\n",
        "\n",
        "# Função para gerar a resposta usando o DeepSeek Qwen 1.5B\n",
        "def gerar_resposta(pergunta):\n",
        "    entrada = tokenizer(pergunta, return_tensors=\"pt\").to(dispositivo)\n",
        "    saida = modelo.generate(**entrada, max_new_tokens=100)\n",
        "    resposta = tokenizer.decode(saida[0], skip_special_tokens=True)\n",
        "    return resposta\n",
        "\n",
        "# Encadeando o Template e a Função de Resposta\n",
        "chatbot_pipeline = template | gerar_resposta\n"
      ],
      "metadata": {
        "id": "4YqeOn5bIVKp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7-4tKVhtIw35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fluxo_conversacional(pergunta):\n",
        "    # Passo 1: Processamento com o prompt\n",
        "    prompt = template.format(pergunta=pergunta)\n",
        "\n",
        "    # Passo 2: Gerar resposta com o modelo\n",
        "    resposta = gerar_resposta(prompt)\n",
        "\n",
        "    # Passo 3: Exibir a resposta\n",
        "    print(\"Chatbot:\", resposta)\n",
        "\n",
        "def chatbot():\n",
        "    while True:\n",
        "        pergunta = input(\"Você: \")\n",
        "        fluxo_conversacional(pergunta)\n",
        "\n",
        "# Rodando o chatbot\n",
        "chatbot()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "R_bWoPl-JBi4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}